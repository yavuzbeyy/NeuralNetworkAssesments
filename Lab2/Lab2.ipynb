{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN8K7VSp4qsZ14lardYJRU0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8wq0cwFnmRK5","executionInfo":{"status":"ok","timestamp":1705766657944,"user_tz":-180,"elapsed":198698,"user":{"displayName":"YAVUZ SELIM SAHIN","userId":"06977586518327549617"}},"outputId":"17e9f8af-d5fb-4422-bda7-73e6ec5a733a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 10175281.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 2346635.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 32278786.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 14498119.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Epoch [1/10], Step [100/938], Loss: 0.6139\n","Epoch [1/10], Step [200/938], Loss: 0.4494\n","Epoch [1/10], Step [300/938], Loss: 0.4013\n","Epoch [1/10], Step [400/938], Loss: 0.3727\n","Epoch [1/10], Step [500/938], Loss: 0.2626\n","Epoch [1/10], Step [600/938], Loss: 0.3453\n","Epoch [1/10], Step [700/938], Loss: 0.1184\n","Epoch [1/10], Step [800/938], Loss: 0.4278\n","Epoch [1/10], Step [900/938], Loss: 0.1217\n","Epoch [2/10], Step [100/938], Loss: 0.4578\n","Epoch [2/10], Step [200/938], Loss: 0.2217\n","Epoch [2/10], Step [300/938], Loss: 0.1538\n","Epoch [2/10], Step [400/938], Loss: 0.0950\n","Epoch [2/10], Step [500/938], Loss: 0.2280\n","Epoch [2/10], Step [600/938], Loss: 0.1852\n","Epoch [2/10], Step [700/938], Loss: 0.1288\n","Epoch [2/10], Step [800/938], Loss: 0.1994\n","Epoch [2/10], Step [900/938], Loss: 0.3439\n","Epoch [3/10], Step [100/938], Loss: 0.2594\n","Epoch [3/10], Step [200/938], Loss: 0.1453\n","Epoch [3/10], Step [300/938], Loss: 0.2015\n","Epoch [3/10], Step [400/938], Loss: 0.0854\n","Epoch [3/10], Step [500/938], Loss: 0.1167\n","Epoch [3/10], Step [600/938], Loss: 0.1550\n","Epoch [3/10], Step [700/938], Loss: 0.1238\n","Epoch [3/10], Step [800/938], Loss: 0.2899\n","Epoch [3/10], Step [900/938], Loss: 0.0944\n","Epoch [4/10], Step [100/938], Loss: 0.0575\n","Epoch [4/10], Step [200/938], Loss: 0.0873\n","Epoch [4/10], Step [300/938], Loss: 0.0114\n","Epoch [4/10], Step [400/938], Loss: 0.2542\n","Epoch [4/10], Step [500/938], Loss: 0.2121\n","Epoch [4/10], Step [600/938], Loss: 0.1547\n","Epoch [4/10], Step [700/938], Loss: 0.0727\n","Epoch [4/10], Step [800/938], Loss: 0.1276\n","Epoch [4/10], Step [900/938], Loss: 0.1311\n","Epoch [5/10], Step [100/938], Loss: 0.0519\n","Epoch [5/10], Step [200/938], Loss: 0.1977\n","Epoch [5/10], Step [300/938], Loss: 0.0604\n","Epoch [5/10], Step [400/938], Loss: 0.0533\n","Epoch [5/10], Step [500/938], Loss: 0.1469\n","Epoch [5/10], Step [600/938], Loss: 0.1762\n","Epoch [5/10], Step [700/938], Loss: 0.1797\n","Epoch [5/10], Step [800/938], Loss: 0.0924\n","Epoch [5/10], Step [900/938], Loss: 0.0626\n","Epoch [6/10], Step [100/938], Loss: 0.0434\n","Epoch [6/10], Step [200/938], Loss: 0.1269\n","Epoch [6/10], Step [300/938], Loss: 0.1613\n","Epoch [6/10], Step [400/938], Loss: 0.0554\n","Epoch [6/10], Step [500/938], Loss: 0.1000\n","Epoch [6/10], Step [600/938], Loss: 0.1054\n","Epoch [6/10], Step [700/938], Loss: 0.0254\n","Epoch [6/10], Step [800/938], Loss: 0.0757\n","Epoch [6/10], Step [900/938], Loss: 0.0516\n","Epoch [7/10], Step [100/938], Loss: 0.0615\n","Epoch [7/10], Step [200/938], Loss: 0.0749\n","Epoch [7/10], Step [300/938], Loss: 0.0352\n","Epoch [7/10], Step [400/938], Loss: 0.0846\n","Epoch [7/10], Step [500/938], Loss: 0.0919\n","Epoch [7/10], Step [600/938], Loss: 0.0086\n","Epoch [7/10], Step [700/938], Loss: 0.0150\n","Epoch [7/10], Step [800/938], Loss: 0.0653\n","Epoch [7/10], Step [900/938], Loss: 0.0085\n","Epoch [8/10], Step [100/938], Loss: 0.0412\n","Epoch [8/10], Step [200/938], Loss: 0.1257\n","Epoch [8/10], Step [300/938], Loss: 0.0426\n","Epoch [8/10], Step [400/938], Loss: 0.1706\n","Epoch [8/10], Step [500/938], Loss: 0.0643\n","Epoch [8/10], Step [600/938], Loss: 0.0354\n","Epoch [8/10], Step [700/938], Loss: 0.1350\n","Epoch [8/10], Step [800/938], Loss: 0.0501\n","Epoch [8/10], Step [900/938], Loss: 0.1056\n","Epoch [9/10], Step [100/938], Loss: 0.0455\n","Epoch [9/10], Step [200/938], Loss: 0.0246\n","Epoch [9/10], Step [300/938], Loss: 0.0120\n","Epoch [9/10], Step [400/938], Loss: 0.0231\n","Epoch [9/10], Step [500/938], Loss: 0.0294\n","Epoch [9/10], Step [600/938], Loss: 0.1045\n","Epoch [9/10], Step [700/938], Loss: 0.0507\n","Epoch [9/10], Step [800/938], Loss: 0.0384\n","Epoch [9/10], Step [900/938], Loss: 0.0845\n","Epoch [10/10], Step [100/938], Loss: 0.0460\n","Epoch [10/10], Step [200/938], Loss: 0.0636\n","Epoch [10/10], Step [300/938], Loss: 0.1844\n","Epoch [10/10], Step [400/938], Loss: 0.0646\n","Epoch [10/10], Step [500/938], Loss: 0.0181\n","Epoch [10/10], Step [600/938], Loss: 0.0245\n","Epoch [10/10], Step [700/938], Loss: 0.0199\n","Epoch [10/10], Step [800/938], Loss: 0.0403\n","Epoch [10/10], Step [900/938], Loss: 0.0951\n","Koniec\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision.datasets import MNIST\n","from torch.utils.data import DataLoader\n","\n","# Define the neural network architecture\n","class SimpleNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleNN, self).__init__()\n","        self.fc1 = nn.Linear(28 * 28, 128)  # Input size: 28x28, Output size: 128\n","        self.fc2 = nn.Linear(128, 64)       # Hidden layer: 128 -> 64\n","        self.fc3 = nn.Linear(64, 10)        # Output layer: 64 -> 10 (10 classes)\n","\n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)  # Flatten the input\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","# Hyperparameters\n","batch_size = 64\n","learning_rate = 0.001\n","num_epochs = 10\n","\n","# Load MNIST dataset and create data loaders\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","train_dataset = MNIST(root='./data', train=True, transform=transform, download=True)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Initialize the model, loss function, and optimizer\n","model = SimpleNN()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        optimizer.zero_grad()  # Zero the gradients\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()       # Backpropagation\n","        optimizer.step()       # Update weights\n","\n","        if (i + 1) % 100 == 0:\n","            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n","\n","print('Koniec')\n"]},{"cell_type":"markdown","source":["Step 2\n"],"metadata":{"id":"7JUecIQ8nitd"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision.datasets import MNIST\n","from torch.utils.data import DataLoader\n","\n","# Define the convolutional neural network architecture\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(torch.relu(self.conv1(x)))\n","        x = self.pool(torch.relu(self.conv2(x)))\n","        x = x.view(x.size(0), -1)\n","        x = torch.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","# Hyperparameters\n","batch_size = 64\n","learning_rate = 0.001\n","num_epochs = 2\n","\n","# Load MNIST dataset and create data loaders\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","train_dataset = MNIST(root='./data', train=True, transform=transform, download=True)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Initialize the model, loss function, and optimizer\n","model = CNN()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        optimizer.zero_grad()  # Zero the gradients\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()       # Backpropagation\n","        optimizer.step()       # Update weights\n","\n","        if (i + 1) % 100 == 0:\n","            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n","\n","print('Koniec!')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T_6V3J3SniJA","executionInfo":{"status":"ok","timestamp":1705766963326,"user_tz":-180,"elapsed":175812,"user":{"displayName":"YAVUZ SELIM SAHIN","userId":"06977586518327549617"}},"outputId":"2c58ad0b-852c-48c8-bf01-c8136c3cd1a4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/2], Step [100/938], Loss: 0.2225\n","Epoch [1/2], Step [200/938], Loss: 0.1307\n","Epoch [1/2], Step [300/938], Loss: 0.1642\n","Epoch [1/2], Step [400/938], Loss: 0.0682\n","Epoch [1/2], Step [500/938], Loss: 0.1523\n","Epoch [1/2], Step [600/938], Loss: 0.0287\n","Epoch [1/2], Step [700/938], Loss: 0.0123\n","Epoch [1/2], Step [800/938], Loss: 0.0249\n","Epoch [1/2], Step [900/938], Loss: 0.0709\n","Epoch [2/2], Step [100/938], Loss: 0.0100\n","Epoch [2/2], Step [200/938], Loss: 0.0145\n","Epoch [2/2], Step [300/938], Loss: 0.0394\n","Epoch [2/2], Step [400/938], Loss: 0.0095\n","Epoch [2/2], Step [500/938], Loss: 0.0064\n","Epoch [2/2], Step [600/938], Loss: 0.0117\n","Epoch [2/2], Step [700/938], Loss: 0.0042\n","Epoch [2/2], Step [800/938], Loss: 0.0282\n","Epoch [2/2], Step [900/938], Loss: 0.0120\n","Koniec!\n"]}]}]}